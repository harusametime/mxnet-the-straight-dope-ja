{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# はじめに\n",
        "\n",
        "これを書き始めようと思うと、カフェインを摂取しないといけませんでした。車に飛び乗って運転を始め、Androidをもって、音声認識を起動させるために\"Okay Google\"と言って、著者であるMuは「ブルーボトルのコーヒーショップへの行き方」と言いました。するとスマートフォンは、すぐに彼の指示内容を文章で表示しました。そしてその指示内容を理解して、我々の意図に合うようにマップのアプリを起動したのです。起動したマップはいくつかのルートを提示しました。そして、各ルートの横には推定の移動時間が示されていました。このような、ちょっとした日々のスマートフォンとのやりとりでさえ、いくつかの機械学習のモデルが利用されています。\n",
        "\n",
        "<!-- \n",
        "Before we could begin writing,\n",
        "the authors of this book, \n",
        "like much of the work force, \n",
        "had to become caffeinated. \n",
        "We hopped in the car and started driving.\n",
        "Having an Android, Alex called out \"Okay Google\",\n",
        "awakening the phone's voice recognition system.\n",
        "Then Mu commanded \"directions to Blue Bottle coffee shop\".\n",
        "The phone quickly displayed the transcription of his command.\n",
        "It also recognized that we were asking for directions \n",
        "and launched the Maps application to fulfill our request. \n",
        "Once launched, the Maps app identified a number of routes. \n",
        "Next to each route, the phone displayed a predicted transit time.\n",
        "While we fabricated this story for pedagogical convenience,\n",
        "it demonstrates that in the span of just a few seconds, \n",
        "our everyday interactions with a smartphone\n",
        "can engage several machine learning models. -->\n",
        "\n",
        "もし、これまで機械学習に取り組んだことがなければ、ここのコンテンツが恐ろしいもののように感じるかもしれません。「これってプログラミング？」とか「機械学習ってどういう意味？」といった疑問をもつと思います。まずはじめに、明確にしておきますが、機械学習のアルゴリズムはすべてコンピュータのプログラミングという形で実装されます。ここでは、他のコンピュータサイエンスの分野でも利用されるようなプログラミング言語とハードウェアを利用しますが、すべてのコンピュータプログラムが機械学習と関係しているわけではありません。2つ目の質問に関しては、機械学習のような広範囲に渡る学問分野を定義することは難しいです。それは、例えば、「数学って何?」といった質問に答えるようなものです。しかし、始めるにあたって、直感的でわかりやすい説明を心がけようと思います。\n",
        "\n\n",
        "<!-- If you've never worked with machine learning before,\n",
        "you might be wondering what the hell we're talking about. \n",
        "You might ask, \"isn't that just programming?\"\n",
        "or \"what does *machine learning* even mean?\"\n",
        "First, to be clear, we implement all machine learning algorithms \n",
        "by writing computer programs. \n",
        "Indeed, we use the same languages and hardware \n",
        "as other fields of computer science,\n",
        "but not all computer programs involve machine learning. \n",
        "In response to the second question, \n",
        "precisely defining a field of study \n",
        "as vast as machine learning is hard. \n",
        "It's a bit like answering, \"what is math?\". \n",
        "But we'll try to give you enough intuition to get started. -->\n",
        "\n\n",
        "## 機械学習の例\n",
        "\n",
        "私達が普段利用しているコンピュータプログラムの多くは、根本的に成立する*第一原理*にもとづいて実装されます。ショッピングカートに商品を追加するとき、e-commerceのアプリを実行して、*shopping cart*という、ユーザIDと商品IDが結びついたデータベースのテーブルにエントリを追加すると思います。このようなプログラムは、実際のお客さんに会ったことがなくても成立する*第一原理*から実装します。もし、簡単なアプリケーションを実装しようとするなら、使えるかどうかよくわからない機械学習を使わないほうが良いです。\n",
        "\n",
        "<!-- Most of the computer programs we interact with every day \n",
        "can be coded up from first principles.\n",
        "When you add an item to your shopping cart, \n",
        "you trigger an e-commerce application to store an entry \n",
        "in a *shopping cart* database table, \n",
        "associating your user ID with the product's ID. \n",
        "We can write such a program from first principles, \n",
        "launch without ever having seen a real customer.\n",
        "When it's this easy to write an application \n",
        "*you should not be using machine learning*. \n",
        " -->\n",
        " \n",
        "これは機械学習のサイエンティストにとって幸運なこともかもしれませんが、多くの問題に対するソリューションは簡単なものではありません。 コーヒーを買いに行く話に戻りましょう。\"Alexa\"、\"Okay Google\"、\"Siri\"といった起動のための言葉に反応するコードを書くことを考えてみます。コンピュータとコードを書くエディタだけをつかって部屋でコードを書いてみましょう。どうやって第一原理に従ってコードを書きますか？考えてみましょう。問題は難しいです。だいたい44,000/秒でマイクからサンプルを集めます。音声の生データに対して、起動の言葉が含まれているかを調べて、YesかNoを正確に対応付けるルールとは何でしょうか？考えは行き詰まってしまうと思いますが心配ありません。このようなプログラムを、ゼロから書く方法はわかりませんが、これこそが機械学習を使う理由なのです。\n",
        "\n",
        "<!-- Fortunately (for the community of ML scientists), however, \n",
        "for many problems, solutions aren't so easy.\n",
        "Returning to our fake story about going to get coffee,\n",
        "imagine just writing a program to respond to a *wake word* \n",
        "like \"Alexa\", \"Okay, Google\" or \"Siri\".\n",
        "Try coding it up in a room by yourself \n",
        "with nothing but a computer and a code editor. \n",
        "How would you write such a program from first principles?\n",
        "Think about it... the problem is hard.\n",
        "Every second, the microphone will collect roughly 44,000 samples.\n",
        "What rule could map reliably from a snippet of raw audio \n",
        "to confident predictions ``{yes, no}`` \n",
        "on whether the snippet contains the wake word?\n",
        "If you're stuck, don't worry. \n",
        "We don't know how to write such a program from scratch either. \n",
        "That's why we use machine learning.  -->\n",
        "\n",
        "![](../img/wake-word.png)\n",
        "\n",
        "ちょっとした考え方を紹介したいと思います。私達が、入力と出力を対応付ける方法をコンピュータに陽に伝えることができなくても、私達自身はそのような認識を行う素晴らしい能力を持っています。言い換えれば、たとえ\"Alexa\"といった言葉を認識するようにコンピュータのプログラムを書けなくても、あなた自身は\"Alexa\"の言葉を認識できます。従って、私達人間は、音声のデータと起動の言葉を含んでいるか否かのラベルをサンプルとして含む巨大なデータセットをつくることができます。機械学習を使うと、起動の言葉を直ちに認識するようなシステムを陽に実装する必要がありません。代わりに、膨大なパラメータで柔軟なプログラムを実装します。パラメータは、プログラムの挙動を柔軟に変更・調整するためのものです。私達はこのプログラムのことをモデルとよんでいます。一般的に、モデルは入力を出力に変換するような機械に過ぎません。上記の例では、モデルは一部の音声を入力として受け取って、それが起動の言葉を含んでいるかどうかを判定できることを期待し、YesかNoを回答として返します。\n",
        "\n",
        "<!-- Here's the trick. \n",
        "Often, even when we don't know how to tell a computer \n",
        "explicitly how to map from inputs to outputs,\n",
        "we are nonetheless capable of performing the cognitive feat ourselves.\n",
        "In other words, even if you don't know *how to program a computer* to recognize the word \"Alexa\",\n",
        "you yourself *are able* to recognize the word \"Alexa\". \n",
        "Armed with this ability, \n",
        "we can collect a huge *data set* containing examples of audio\n",
        "and label those that *do* and that *do not* contain the wake word.\n",
        "In the machine learning approach, we do not design a system *explicitly* to recognize \n",
        "wake words right away. \n",
        "Instead, we define a flexible program with a number of *parameters*. \n",
        "These are knobs that we can tune to change the behavior of the program.\n",
        "We call this program a model.\n",
        "Generally, our model is just a machine that transforms its input into some output.\n",
        "In this case, the model receives as *input* a snippet of audio,\n",
        "and it generates as output an answer ``{yes, no}``,\n",
        "which we hope reflects whether (or not) the snippet contains the wake word. -->\n",
        "\n",
        "もし私達が、正しいモデルを選ぶことができれば、そのモデルは\"Alexa\"という言葉を聞いたときに\"Yes\"と出力するペアが必要になります。\"Yes\"という言葉は\"Apricot\"に置き換えることもできるでしょう。\"Alexa\"を認識する機能は\"Apricot\"を認識する機能は非常に似たようなものであり、同じモデルが適用できると思うかもしれません。しかし、入出力が変われば根本的に別のモデルを必要とする場合もあります。例えば、画像とラベルを対応付けるタスクと、英語と中国語を対応付けるタスクには、異なるモデルを利用する必要があるでしょう。\n",
        "\n",
        "<!-- \n",
        "If we choose the right kind of model, \n",
        "then there should exist one setting of the knobs\n",
        "such that the model fires ``yes`` every time it hears the word \"Alexa\".\n",
        "There should also be another setting of the knobs that might fire ``yes`` \n",
        "on the word \"Apricot\". \n",
        "We expect that the same model should apply to \"Alexa\" recognition and \"Apricot\" recognition\n",
        "because these are similar tasks. \n",
        "However, we might need a different model to deal with fundamentally different inputs or outputs. \n",
        "For example, we might choose a different sort of machine to map from images to captions,\n",
        "or from English sentences to Chinese sentences.  -->\n",
        "\n",
        "想像できるとは思いますが、\"Alexa\"と\"Yes\"のようなペアがランダムなものであれば、モデルは\"Alexa\"も\"Apricot\"も他の英語も何も認識できないでしょう。Deep Learningという言葉の中の*Learning*というのは、*学習の期間*において、複数のペアを上手く使って、モデルの挙動を更新していくことを指します。その学習のプロセスというのは以下のようなものです。\n",
        "\n",
        "1. まず最初にランダムに初期化されたモデルから始めます。このモデルは最初は使い物になりません\n",
        "1. ラベルデータを取り込みます（例えば、部分的な音声データと対応するYes/Noのラベルです）\n",
        "1. そのペアを利用してモデルを改善します\n",
        "1. モデルが良いものになるまで繰り返します\n",
        "\n",
        "<!-- As you might guess, if we just set the knobs randomly,\n",
        "the model will probably recognize neither \"Alexa\", \"Apricot\",\n",
        "nor any other English word. \n",
        "Generally, in deep learning, the *learning*\n",
        "refers precisely \n",
        "to updating the model's behavior (by twisting the knobs)\n",
        "over the course of a *training period*. \n",
        "\n",
        "The training process usually looks like this:\n",
        "\n",
        "1. Start off with a randomly initialized model that can't do anything useful.\n",
        "1. Grab some of your labeled data (e.g. audio snippets and corresponding ``{yes,no}`` labels)\n",
        "1. Tweak the knobs so the model sucks less with respect to those examples\n",
        "1. Repeat until the model is awesome. -->\n",
        "\n",
        "![](../img/ml-loop.png)\n",
        "\n",
        "まとめると、起動の言葉を認識できるようなコードを直接書くよりも、もしラベル付きのデータをたくさん集められるなら、その認識機能を学習するようなコードを書くべきです。これは、プログラムの挙動をデータセットで決める、つまりデータでプログラムをするようなものだと考えることができます。\n",
        "\n",
        "私達は、以下に示すようなネコとイヌの画像サンプルを大量に集めて、機械学習を利用することで、ネコ認識器をプログラムすることができます。\n",
        "\n",
        "<!-- To summarize, rather than code up a wake word recognizer, \n",
        "we code up a program that can *learn* to recognize wake words, \n",
        "*if we present it with a large labeled dataset*.\n",
        "You can think of this act\n",
        "of determining a program's behavior by presenting it with a dataset\n",
        "as *programming with data*.\n",
        "\n",
        "We can 'program' a cat detector by providing our machine learning system with many examples of cats and dogs, such as the images below:\n",
        " -->\n",
        " \n",
        "|![](../img/cat1.jpg)|![](../img/cat2.jpg)|![](../img/dog1.jpg)|![](../img/dog2.jpg)|\n",
        "|:---------------:|:---------------:|:---------------:|:---------------:|\n",
        "|ネコ|ネコ|イヌ|イヌ|\n",
        "\n",
        "このケースでは、認識器はネコであれば正の値、イヌであれば負の値、どちらかわからない場合はゼロを出力するように学習するでしょう。しかし、そのイヌとネコを識別する機械学習の境界を直接するプログラミングするわけではないです。\n",
        "\n",
        "<!-- \n",
        "This way the detector will eventually learn to emit a very large positive number if it's a cat, a very large negative number if it's a dog, and something closer to zero if it isn't sure, but this is just barely scratching the surface of what machine learning can do.\n",
        " -->\n",
        "\n",
        "## 機械学習の多機能性\n",
        "\n",
        "これは機械学習のもとになる重要な話ですが、ある特定の挙動に関して直接プログラムとして書くよりは、私達人間が実現してきたように、その挙動を改善していく能力をコードとして実装します。この基本的な考え方は様々な形式で実装されます。機械学習は、多くの異なる分野で利用されており、様々なタイプのモデルに関係し、新しいアルゴリズムに応じてモデルを更新してきました。機械学習の例として、音声認識の問題に対する*教師あり学習*について述べたいと思います。\n",
        "<!-- \n",
        "This is the core idea behind machine learning:\n",
        "Rather than code programs with fixed behavior,\n",
        "we design programs with the ability to improve\n",
        "as they acquire more experience. \n",
        "This basic idea can take many forms.\n",
        "Machine learning can address many different application domains, \n",
        "involve many different types of models,\n",
        "and update them according to many different learning algorithms.\n",
        "In this particular case, we described an instance of *supervised learning* \n",
        "applied to a problem in automated speech recognition. \n",
        " -->\n",
        "\n",
        "単純なルールベースのシステムが上手く行かなかったり、構築が非常に難しい状況において、データを利用して上手く動くようにした多くのツール群が機械学習と言えます。例えば、機械学習の技術は検索エンジン、自動運転、機械翻訳、医療診断、スパムフィルタ、ゲームプレイ(チェスや囲碁)、顔認識、データマッチング、保険料の計算、写真の加工フィルタなどに利用されています。\n",
        "<!-- \n",
        "Machine Learning is a versatile set of tools that lets you work with data in many different situations where simple rule-based systems would fail or might be very difficult to build. Due to its versatility, machine learning can be quite confusing to newcomers.\n",
        "For example, machine learning techniques are already widely used\n",
        "in applications as diverse as search engines, self driving cars, \n",
        "machine translation, medical diagnosis, spam filtering, \n",
        "game playing (*chess*, *go*), face recognition, \n",
        "data matching, calculating insurance premiums, and adding filters to photos.  -->\n",
        "\n",
        "表面上は違うように見える問題も共通の構造をもっていて、Deep Learningで扱えるものもあります。これらの問題は、挙動を直接的にコードで記述できないけれども、データでプログラムする、という点においては似通っています。こういったプログラムは数学という共通言語によってつながっています。このコンテンツでは、数学の記述に関して最小限にとどめつつ、他の機械学習やニューラルネットワークの書籍とは違って、実際の例とコードにもとづいて説明をしたいと思います。\n",
        "\n",
        "<!-- Despite the superficial differences between these problems many of them share a common structure\n",
        "and are addressable with deep learning tools. \n",
        "They're mostly similar because they are problems where we wouldn't be able to program their behavior directly in code, \n",
        "but we can *program them with data*.\n",
        "Often times the most direct language for communicating these kinds of programs is *math*. \n",
        "In this book, we'll introduce a minimal amount of mathematical notation,\n",
        "but unlike other books on machine learning and neural networks,\n",
        "we'll always keep the conversation grounded in real examples and real code.\n",
        " -->\n"
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 機械学習の基礎\n",
        "\n",
        "起動のための言葉を認識するタスクを考えたとき、音声データとラベルからなるデータセットを準備します。そこで、音声からラベルを推定する機械学習モデルをどうやって学習させるかを記述するかもしれません。サンプルからラベルを推定するこのセットアップは機械学習の一種で、*教師あり学習*と呼ばれるものです。Deep Learningにおいても多くのアプローチがありますが、それについては以降の章で述べたいと思います。機械学習を進めるために、以下の４つのことが必要になります。\n",
        "\n",
        "1. データ\n",
        "2. データを変換して推定するためのモデル\n",
        "3. そのモデルが上手く行っているかどうかを測るためのロス関数\n",
        "4. ロス関数を最小化するような、モデルのパラメータを探すアルゴリズム\n",
        "\n",
        "<!-- \n",
        "When we considered the task of recognizing wake-words, \n",
        "we put together a dataset consisting of snippets and labels.\n",
        "We then described (albeit abstractly) \n",
        "how you might train a machine learning model\n",
        "to predict the label given a snippet. \n",
        "This set-up, predicting labels from examples, is just one flavor of ML \n",
        "and it's called *supervised learning*. \n",
        "Even within deep learning, there are many other approaches, \n",
        "and we'll discuss each in subsequent sections. \n",
        "To get going with machine learning, we need four things: \n",
        "     \n",
        "1. Data\n",
        "2. A model of how to transform the data\n",
        "3. A loss function to measure how well we're doing\n",
        "4. An algorithm to tweak the model parameters such that the loss function is minimized\n",
        "      -->\n",
        "      \n",
        "### データ\n",
        "\n",
        "一般的に、多くのデータを持っていれば持っているほど、問題を簡単に解くことができます。多くのデータを持っているなら、より性能の良いモデルを構築することができるからです。データはDeep Learningの発達に大きく貢献し、現在の多くのDeep Learningモデルは大規模なデータセットなしには動きません。以下では、機械学習を実践するうえで扱うことが多いデータの例を示します。\n",
        "<!-- \n",
        "Generally, the more data we have, the easier our job becomes. \n",
        "When we have more data, we can train more powerful models. Data is at the heart of the resurgence of deep learning and many of most exciting models in deep learning don't work without large data sets. Here are some examples of the kinds of data machine learning practitioners often engage with: -->\n",
        "\n",
        "**画像** スマートフォンで撮影されたり、Webで収集された画像、衛星画像、超音波やCTやMRIなどのレントゲン画像など\n",
        "\n",
        "**テキスト** Eメール、学校でのエッセイ、tweet、ニュース記事、医者のメモ、書籍、翻訳文のコーパスなど\n",
        "\n",
        "**音声** Amazon Echo、iPhone、Androidのようなスマートデバイスに送られる音声コマンド、音声つき書籍、通話、音楽など\n",
        "\n",
        "**動画** テレビや映画、Youtubeのビデオ、携帯電話の撮影、自宅の監視カメラ映像、複数カメラによる追跡、など\n",
        "\n",
        "**構造化データ** ウェブページ、電子カルテ、レンタカーの記録、デジタルな請求書など\n",
        "\n",
        "<!--      \n",
        "* **Images:** Pictures taken by smartphones or harvested from the web, satellite images, photographs of medical conditions, ultrasounds, and radiologic images like CT scans and MRIs, etc. \n",
        "* **Text:** Emails, high school essays, tweets, news articles, doctor's notes, books, and corpora of translated sentences, etc.\n",
        "* **Audio:** Voice commands sent to smart devices like Amazon Echo, or iPhone or Android phones, audio books, phone calls, music recordings, etc.\n",
        "* **Video:** Television programs and movies, YouTube videos, cell phone footage, home surveillance, multi-camera tracking, etc.\n",
        "* **Structured data:** Webpages, electronic medical records, car rental records, electricity bills, etc.\n",
        "      -->\n",
        "      \n",
        "### モデル\n",
        "\n",
        "通常、データは私達が達成しようとすることとは大きく異なっています。例えば、人間の写真を持っていて、そこに映る人たちが幸せかどうかを知りたいとします。そのために、高解像度の画像から幸福度を出力するようなモデルを必要とすると思います。簡単な問題がシンプルなモデルで解決できるかもしれないのに対し、このケースではいくつかの問いを投げかけることになります。幸福度を求めるためには、その検出器が数百、数千の低レベルな特徴（画像の場合はピクセル単位）をかなり抽象的な幸福度に変換する必要があります。そして、正しいモデルを選ぶのは難しく、異なるデータセットには異なるモデルが適しています。このコンテンツでは、モデルとしてDeep Neural Networksに着目します。これらのモデルは最初（入力）から最後（出力）までつながった、たくさんのデータ変換で構成されています。従って、*Deep Learning*と呼ばれているのです。*Deep Nets*、つまりDeep Learningのモデルについて議論するときは、比較的シンプルで浅いモデルを議論するようにしたいと思います。\n",
        "\n",
        "<!-- \n",
        "Usually the data looks quite different from what we want to accomplish with it. \n",
        "For example, we might have photos of people and want to know whether they appear to be happy. \n",
        "We might desire a model capable of ingesting a high-resolution image and outputting a happiness score. \n",
        "While some simple problems might be addressable with simple models, we're asking a lot in this case. \n",
        "To do its job, our happiness detector needs to transform hundreds of thousands of low-level features (pixel values) \n",
        "into something quite abstract on the other end (happiness scores). \n",
        "Choosing the right model is hard, and different models are better suited to different datasets. \n",
        "In this book, we'll be focusing mostly on deep neural networks. \n",
        "These models consist of many successive transformations of the data that are chained together top to bottom, \n",
        "thus the name *deep learning*. \n",
        "On our way to discussing deep nets, we'll also discuss some simpler, shallower models.  -->\n",
        "\n\n",
        "###  ロス関数\n",
        "\n",
        "モデルが良いかどうかを評価するためには、モデルの出力と実際の正解を比較する必要があります。ロス関数は、その出力が**悪い**ことを評価する方法です。例えば、画像から患者の心拍数を予測するモデルを学習する場合を考えます。そのモデルが心拍数は100bpmだと推定して、実際は60bpmが正解だったときには、そのモデルに対して推定結果が悪いことを伝えなくてはなりません。\n",
        "\n",
        "同様に、Eメールがスパムである確率を予測するモデルを作りたいとき、その予測が上手く行っていなかったら、そのモデルに伝える方法が必要になります。一般的に、機械学習の*学習*と呼ばれる部分はロス関数を最小化することです。通常、モデルはたくさんのパラメータをもっています。パラメータの最適な値というのは、私達が*学習*で必要とするものであり、観測したデータのなかの*学習データ*の上でロスを最小化することによって得られます。残念ながら、学習データ上でいくら上手くロス関数を最小化しても、いまだ見たことがないテストデータにおいて、学習したモデルがうまくいくという保証はありません。従って、以下の2つの指標をチェックする必要があります。\n",
        "\n",
        "<!-- \n",
        "To assess how well we're doing we need to compare the output from the model with the truth. \n",
        "Loss functions give us a way of measuring how *bad* our output is.\n",
        "For example, say we trained a model to infer a patient's heart rate from images.\n",
        "If the model predicted that a patient's heart rate was 100bpm, \n",
        "when the ground truth was actually 60bpm,\n",
        "we need a way to communicate to the model that it's doing a lousy job.\n",
        "\n\n",
        "Similarly if the model was assigning scores to emails indicating the probability that they are spam,\n",
        "we'd need a way of telling the model when its predictions are bad. \n",
        "Typically the *learning* part of machine learning consists of minimizing this loss function.\n",
        "Usually, models have many parameters. \n",
        "The best values of these parameters is what we need to 'learn', typically by minimizing the loss incurred on a *training data*\n",
        "of observed data. \n",
        "Unfortunately, doing well on the training data \n",
        "doesn't guarantee that we will do well on (unseen) test data, \n",
        "so we'll want to keep track of two quantities. -->\n",
        "\n",
        " * **学習誤差:** これは、学習データ上でロスを最小化してモデルを学習したときの、学習データにおける誤差です。これは、実際の試験に備えて、学生が練習問題をうまく説いているようなものです。その結果は、実際の試験の結果が良いかどうかを期待させますが、最終試験での成功を保証するものではありません。\n",
        "\n",
        "<!--  \n",
        " This is the error on the dataset used to train our model by minimizing the loss on the training set. This is equivalent to doing well on all the practice exams that a student might use to prepare for the real exam. The results are encouraging, but by no means guarantee success on the final exam. -->\n",
        " \n",
        " * **テスト誤差:** これは、見たことのないテストデータに対する誤差で、学習誤差とは少し異なっています。見たことのないデータに対して、モデルが対応（汎化）できないとき、その状態を\n",
        "*overfitting* (過適合)と呼びます。実生活でも、練習問題に特化して準備したにもかかわらず、本番の試験で失敗するというのと似ています。\n",
        "\n",
        "<!--  This is the error incurred on an unseen test set. This can deviate quite a bit from the training error. This condition, when a model fails to generalize to unseen data, is called *overfitting*. In real-life terms, this is the equivalent of screwing up the real exam despite doing well on the practice exams.\n",
        "      -->\n",
        "\n",
        "### 最適化アルゴリズム    \n",
        "\n",
        "最終的にロスを最小化するということは、モデルとそのロス関数に対して、ロスを最小化するようなモデルのパラメータを探索するということになります。ニューラルネットワークにおける最も有名な最適化アルゴリズムは、最急降下法と呼ばれる方法にもとづいています。端的に言えば、パラメーラを少しだけ動かしたら、学習データに対するロスがどのような方向に変化するかを、パラメータごとに見ることになります。ロスが小さくなる方向へパラメータを更新します。\n",
        "\n",
        "次の節では、機械学習のいくつかの種類について詳細を議論します。まず、機械学習の目的、言い換えれば機械学習ができることに関して、そのリストを紹介します。その目的は、目的を達成するための手段、いわば学習やデータの種類と補完的な位置づけであることに気をつけてください。以下のリストは、ひとまず、読者の好奇心をそそり、問題について話し合うための共通言語を理解することを目的としています。より多くの問題については、追って紹介をしていきたいと思います。\n",
        "\n",
        "<!-- \n",
        "Finally, to minimize the loss, we'll need some way of taking the model and its loss functions, \n",
        "and searching for a set of parameters that minimizes the loss.\n",
        "The most popular optimization algorithms for work on neural networks\n",
        "follow an approach called gradient descent. \n",
        "In short, they look to see, for each parameter which way the training set loss would move if you jiggled the parameter a little bit. They then update the parameter in the direction that reduces the loss.\n",
        "\nIn the following sections, we will discuss a few types of machine learning in some more detail. We begin with a list of *objectives*, i.e. a list of things that machine learning can do. Note that the objectives are complemented with a set of techniques of *how* to accomplish them, i.e. training, types of data, etc. The list below is really only sufficient to whet the readers' appetite and to give us a common language when we talk about problems. We will introduce a larger number of such problems as we go along.  -->"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 教師あり学習\n",
        "\n",
        "教師あり学習は、入力データが与えられて、何らかの*対象*を予測するものです。その対象というのは、*ラベル*と呼ばれていて、*y*で表現することが多いです。入力データ点は、*example* や *instance* と呼ばれることがあり、$\\boldsymbol{x}$で表現されることが多いです。\n",
        "教師あり学習の目的は、入力$\\boldsymbol{x}$から予測$f_{\\theta}(\\boldsymbol{x})$を得るようなモデル$f_\\theta$を生成することです。\n",
        "\n",
        "<!-- Supervised learning addresses the task of predicting *targets* given input data.\n",
        "The targets, also commonly called *labels* are generally denoted *y*.\n",
        "The input data points, also commonly called *examples* or *instances*, are typically denoted $\\boldsymbol{x}$.\n",
        "The goal is to produce a model $f_\\theta$ that maps an input $\\boldsymbol{x}$ to a prediction $f_{\\theta}(\\boldsymbol{x})$ -->\n",
        "\n",
        "この説明を例を使って具体的に説明したいと思います。ヘルスケアの分野で仕事をしているとき、患者が心臓発作を起こすかどうかを予測したいと思います。実際に観測した内容、この場合、*心臓発作*か*心臓発作でない*かがラベル$y$です。入力データ$\\boldsymbol{x}$は、心拍数や、最高、最低の血圧といった、いわゆるバイタルサインというものになるでしょう。\n",
        "\n",
        "<!-- To ground this description in a concrete example,\n",
        "if we were working in healthcare,\n",
        "then we might want to predict whether or not a patient would have a heart attack. \n",
        "This observation, *heart attack* or *no heart attack*,\n",
        "would be our label $y$.\n",
        "The input data $\\boldsymbol{x}$ might be vital signs such as heart rate, diastolic and systolic blood pressure, etc.  -->\n",
        "\n",
        "教師あり学習における、教師するというのは、パラメータ$\\theta$を選ぶためのもので、私達が教師となって、ラベルの付いた入力データ($\\boldsymbol{x}_i, y_i$)とモデルを与えます。\n",
        "各データ$\\boldsymbol{x}_i$は正しいラベルと対応しています。\n",
        "\n",
        "<!-- The supervision comes into play because for choosing the parameters $\\theta$, we (the supervisors) provide the model with a collection of *labeled examples* ($\\boldsymbol{x}_i, y_i$), where each example $\\boldsymbol{x}_i$ is matched up against it's correct label. -->\n",
        "\n",
        "In probabilistic terms, we typically are interested estimating \n",
        "the conditional probability $P(y|x)$. \n",
        "While it's just one among several approaches to machine learning, \n",
        "supervised learning accounts for the majority of machine learning in practice. \n",
        "Partly, that's because many important tasks \n",
        "can be described crisply as estimating the probability of some unknown given some available evidence: \n",
        "\n",
        "* **Predict cancer vs not cancer, given a CT image.** \n",
        "* **Predict the correct translation in French, given a sentence in English.**\n",
        "* **Predict the price of a stock next month based on this month's financial reporting data.**\n",
        "\n",
        "Even with the simple description \"predict targets from inputs\" \n",
        "supervised learning can take a great many forms and require a great many modeling decisions,\n",
        "depending on the type, size, and the number of inputs and outputs. \n",
        "For example, we use different models to process sequences (like strings of text or time series data)\n",
        "and for processing fixed-length vector representations.\n",
        "We'll visit many of these problems in depth throughout the first 9 parts of this book. \n",
        "\n",
        "Put plainly, the learning process looks something like this.\n",
        "Grab a big pile of example inputs, selecting them randomly.\n",
        "Acquire the ground truth labels for each.\n",
        "Together, these inputs and corresponding labels (the desired outputs)\n",
        "comprise the training set. \n",
        "We feed the training dataset into a supervised learning algorithm.\n",
        "So here the *supervised learning algorithm* is a function that takes as input a dataset,\n",
        "and outputs another function, *the learned model*. \n",
        "Then, given a learned model, \n",
        "we can take a new previously unseen input, and predict the corresponding label.\n",
        "\n",
        "![](../img/supervised-learning.png)\n",
        "\n\n\n",
        "### Regression\n",
        "\n",
        "Perhaps the simplest supervised learning task to wrap your head around is Regression.\n",
        "Consider, for example a set of data harvested \n",
        "from a database of home sales.\n",
        "We might construct a table, where each row corresponds to a different house, \n",
        "and each column corresponds to some relevant attribute,\n",
        "such as the square footage of a house, the number of bedrooms, the number of bathrooms,\n",
        "and the number of minutes (walking) to the center of town. \n",
        "Formally, we call one row in this dataset a *feature vector*,\n",
        "and the object (e.g. a house) it's associated with an *example*.\n",
        "\n",
        "If you live in New York or San Francisco, and you are not the CEO of Amazon, Google, Microsoft, or Facebook, \n",
        "the (sq. footage, no. of bedrooms, no. of bathrooms, walking distance) feature vector for your home \n",
        "might look something like: $[100, 0, .5, 60]$. \n",
        "However, if you live in Pittsburgh, \n",
        "it might look more like $[3000, 4, 3, 10]$.\n",
        "Feature vectors like this are essential for all the classic machine learning problems.\n",
        "We'll typically denote the feature vector for any one example $\\mathbf{x_i}$\n",
        "and the set of feature vectors for all our examples $X$.\n",
        "\n",
        "What makes a problem *regression* is actually the outputs.\n",
        "Say that you're in the market for a new home, \n",
        "you might want to estimate the fair market value of a house,\n",
        "given some features like these. \n",
        "The target value, the price of sale, is a *real number*.\n",
        "We denote any individual target $y_i$ (corresponding to example $\\mathbf{x_i}$) \n",
        "and the set of all targets $\\mathbf{y}$ (corresponding to all examples X). \n",
        "When our targets take on arbitrary real values in some range, \n",
        "we call this a regression problem. \n",
        "The goal of our model is to produce predictions (guesses of the price, in our example)\n",
        "that closely approximate the actual target values.  \n",
        "We denote these predictions $\\hat{y}_i$ \n",
        "and if the notation seems unfamiliar, then just ignore it for now. \n",
        "We'll unpack it more thoroughly in the subsequent chapters.\n",
        "\n\n",
        "Lots of practical problems are well-described regression problems. \n",
        "Predicting the rating that a user will assign to a movie is a regression problem,\n",
        "and if you designed a great algorithm to accomplish this feat in 2009,\n",
        "you might have won the [$1 million Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize).\n",
        "Predicting the length of stay for patients in the hospital is also a regression problem.\n",
        "A good rule of thumb is that any *How much?* or *How many?* problem should suggest regression.\n",
        "* \"How many hours will this surgery take?\"... *regression*\n",
        "* \"How many dogs are in this photo?\" ... *regression*.\n",
        "However, if you can easily pose your problem as \"Is this a ___?\", then it's likely, classification, a different fundamental problem type that we'll cover next.\n",
        "\n",
        "Even if you've never worked with machine learning before, \n",
        "you've probably worked through a regression problem informally. \n",
        "Imagine, for example, that you had your drains repaired \n",
        "and that your contractor, spent $x_1=3$ hours removing gunk from your sewage pipes.\n",
        "Then she sent you a bill of $y_1 = \\$350$. \n",
        "Now imagine that your friend hired the same contractor for $x_2 = 2$ hours \n",
        "and that she received a bill of $y_2 = \\$250$. \n",
        "If someone then asked you how much to expect on their upcoming gunk-removal invoice\n",
        "you might make some reasonable assumptions, \n",
        "such as more hours worked costs more dollars.\n",
        "You might also assume that there's some base charge and that the contractor then charges per hour.\n",
        "If these assumptions held, then given these two data points, \n",
        "you could already identify the contractor's pricing structure: \n",
        "\\$100 per hour plus \\$50 to show up at your house. \n",
        "If you followed that much then you already understand the high-level idea behind linear regression.\n",
        "\n",
        "In this case, we could produce the parameters that exactly matched the contractor's prices. \n",
        "Sometimes that's not possible, e.g., if some of the variance owes to some factors besides your two features.\n",
        "In these cases, we'll try to learn models that minimize the distance between our predictions and the observed values.\n",
        "In most of our chapters, we'll focus on one of two very common losses, the [L1 loss](http://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L1Loss) where $l(y,y') = \\sum_i |y_i-y_i'|$ and the [L2 loss](http://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L2Loss) where $l(y,y') = \\sum_i (y_i - y_i')^2$.\n",
        "As we will see later, the $L_2$ loss corresponds to the assumption that our data was corrupted by Gaussian noise, whereas the $L_1$ loss corresponds to an assumption of noise from a Laplace distribution. \n",
        "\n",
        "### Classification\n",
        "\n",
        "While regression models are great for addressing *how many?* questions, \n",
        "lots of problems don't bend comfortably to this template. For example, \n",
        "a bank wants to add check scanning to their mobile app. \n",
        "This would involve the customer snapping a photo of a check with their smartphone's camera \n",
        "and the machine learning model would need to be able to automatically understand text seen in the image. \n",
        "It would also need to understand hand-written text to be even more robust. \n",
        "This kind of system is referred to as optical character recognition (OCR), \n",
        "and the kind of problem it solves is called a classification. \n",
        "It's treated with a distinct set of algorithms than those that are used for regression. \n",
        "\n",
        "In classification, we want to look at a feature vector, like the pixel values in an image, \n",
        "and then predict which category (formally called *classes*), \n",
        "among some set of options, an example belongs.\n",
        "For hand-written digits, we might have 10 classes, \n",
        "corresponding to the digits 0 through 9.\n",
        "The simplest form of classification is when there are only two classes, \n",
        "a problem which we call binary classification.\n",
        "For example, our dataset $X$ could consist of images of animals \n",
        "and our *labels* $Y$ might be the classes $\\mathrm{\\{cat, dog\\}}$.\n",
        "While in regression, we sought a regressor to output a real value $\\hat{y}$,\n",
        "in classification, we seek a *classifier*, whose output $\\hat{y}$ is the predicted class assignment.\n",
        "\n",
        "For reasons that we'll get into as the book gets more technical, it's pretty hard to optimize a model that can only output a hard categorical assignment, e.g. either *cat* or *dog*. \n",
        "It's a lot easier instead to express the model in the language of probabilities. \n",
        "Given an example $x$, the model assigns a probability $\\hat{y}_k$ to each label $k$. \n",
        "Because these are probabilities, they need to be positive numbers and add up to $1$. \n",
        "This means that we only need $K-1$ numbers to give the probabilities of $K$ categories.\n",
        "This is easy to see for binary classification. \n",
        "If there's a 0.6 (60%) probability that an unfair coin comes up heads, \n",
        "then there's a 0.4 (40%) probability that it comes up tails. \n",
        "Returning to our animal classification example, a classifier might see an image \n",
        "and output the probability that the image is a cat $\\Pr(y=\\mathrm{cat}\\mid x) = 0.9$.\n",
        "We can interpret this number by saying that the classifier is 90% sure that the image depicts a cat. \n",
        "The magnitude of the probability for the predicted class is one notion of confidence. \n",
        "It's not the only notion of confidence and we'll discuss different notions of uncertainty in more advanced chapters.\n",
        "\n",
        "When we have more than two possible classes, we call the problem *multiclass classification*.\n",
        "Common examples include hand-written character recognition `[0, 1, 2, 3 ... 9, a, b, c, ...]`. \n",
        "While we attacked regression problems by trying to minimize the L1 or L2 loss functions,\n",
        "the common loss function for classification problems is called cross-entropy.\n",
        "In `MXNet Gluon`, the corresponding loss function can be found [here](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss). \n",
        "\n",
        "Note that the most likely class is not necessarily the one that you're going to use for your decision. Assume that you find this beautiful mushroom in your backyard:\n",
        "\n",
        "|![](../img/death_cap.jpg)|\n",
        "|:-------:|\n",
        "|Death cap - do not eat!|\n",
        "\n",
        "Now, assume that you built a classifier and trained it \n",
        "to predict if a mushroom is poisonous based on a photograph.\n",
        "Say our poison-detection classifier outputs $\\Pr(y=\\mathrm{death cap}\\mid\\mathrm{image}) = 0.2$. \n",
        "In other words, the classifier is 80% confident that our mushroom *is not* a death cap. \n",
        "Still, you'd have to be a fool to eat it. \n",
        "That's because the certain benefit of a delicious dinner isn't worth a 20% chance of dying from it. \n",
        "In other words, the effect of the *uncertain risk* by far outweighs the benefit. \n",
        "Let's look at this in math. Basically, we need to compute the expected risk that we incur, i.e. we need to multiply the probability of the outcome with the benefit (or harm) associated with it:\n",
        "\n",
        "$$L(\\mathrm{action}\\mid x) = \\mathbf{E}_{y \\sim p(y\\mid x)}[\\mathrm{loss}(\\mathrm{action},y)]$$\n",
        "\n",
        "Hence, the loss $L$ incurred by eating the mushroom is $L(a=\\mathrm{eat}\\mid x) = 0.2 * \\infty + 0.8 * 0 = \\infty$, whereas the cost of discarding it is $L(a=\\mathrm{discard}\\mid x) = 0.2 * 0 + 0.8 * 1 = 0.8$. \n",
        "\n",
        "We got lucky: as any mycologist would tell us, the above actually *is* a death cap.\n",
        "Classification can get much more complicated than just binary, multiclass, of even multi-label classification.\n",
        "For instance, there are some variants of classification for addressing hierarchies. \n",
        "Hierarchies assume that there exist some relationships among the many classes.\n",
        "So not all errors are equal - we prefer to misclassify to a related class than to a distant class.\n",
        "Usually, this is referred to as *hierarchical classification*. \n",
        "One early example is due to [Linnaeus](https://en.wikipedia.org/wiki/Carl_Linnaeus),\n",
        "who organized the animals in a hierarchy.. \n",
        "\n",
        "![](../img/taxonomy.jpg)\n",
        "\n",
        "In the case of animal classification, it might not be so bad to mistake a poodle for a schnauzer, \n",
        "but our model would pay a huge penalty if it confused a poodle for a dinosaur. \n",
        "What hierarchy is relevant might depend on how you plan to use the model.\n",
        "For example, rattle snakes and garter snakes might be close on the phylogenetic tree, \n",
        "but mistaking a rattler for a garter could be deadly. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tagging\n",
        "\n",
        "Some classification problems don't fit neatly into the binary or multiclass classification setups. \n",
        "For example, we could train a normal binary classifier to distinguish cats from dogs.\n",
        "Given the current state of computer vision, \n",
        "we can do this easily, with off-the-shelf tools.\n",
        "Nonetheless, no matter how accurate our model gets, we might find ourselves in trouble when the classifier encounters an image like this: \n",
        "\n",
        "![](../img/catdog.jpg)\n",
        "\n",
        "As you can see, there's a cat in the picture. \n",
        "There is also a dog, a tire, some grass, a door, concrete, \n",
        "rust, individual grass leaves, etc. \n",
        "Depending on what we want to do with our model ultimately, \n",
        "treating this as a binary classification problem \n",
        "might not make a lot of sense. \n",
        "Instead, we might want to give the model the option \n",
        "of saying the image depicts a cat *and* a dog, \n",
        "or *neither* a cat *nor* a dog. \n",
        "\n",
        "The problem of learning to predict classes \n",
        "that are *not mutually exclusive* \n",
        "is called multi-label classification. \n",
        "Auto-tagging problems are typically best described \n",
        "as multi-label classification problems. \n",
        "Think of the tags people might apply to posts on a tech blog, \n",
        "e.g., \"machine learning\", \"technology\", \"gadgets\", \n",
        "\"programming languages\", \"linux\", \"cloud computing\", \"AWS\". \n",
        "A typical article might have 5-10 tags applied \n",
        "because these concepts are correlated. \n",
        "Posts about \"cloud computing\" are likely to mention \"AWS\" \n",
        "and posts about \"machine learning\" could also deal with \"programming languages\". \n",
        "\n",
        "We also have to deal with this kind of problem when dealing with the biomedical literature,\n",
        "where correctly tagging articles is important \n",
        "because it allows researchers to do exhaustive reviews of the literature. \n",
        "At the National Library of Medicine, a number of professional annotators \n",
        "go over each article that gets indexed in PubMed \n",
        "to associate each with the relevant terms from MeSH, \n",
        "a collection of roughly 28k tags. \n",
        "This is a time-consuming process and the annotators typically have a one year lag between archiving and tagging. Machine learning can be used here to provide provisional tags \n",
        "until each article can have a proper manual review. \n",
        "Indeed, for several years, the BioASQ organization has [hosted a competition](http://bioasq.org/) \n",
        "to do precisely this.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search and ranking\n",
        "\n",
        "Sometimes we don't just want to assign each example to a bucket or to a real value. In the field of information retrieval, we want to impose a ranking on a set of items. Take web search for example, the goal is less to determine whether a particular page is relevant for a query, but rather, which one of the plethora of search results should be displayed for the user. We really care about the ordering of the relevant search results and our learning algorithm needs to produce ordered subsets of elements from a larger set. In other words, if we are asked to produce the first 5 letters from the alphabet, there is a difference between returning ``A B C D E`` and ``C A B E D``. Even if the result set is the same, the ordering within the set matters nonetheless.\n",
        "\n",
        "One possible solution to this problem is to score every element in the set of possible sets along with a corresponding relevance score and then to retrieve the top-rated elements. [PageRank](https://en.wikipedia.org/wiki/PageRank) is an early example of such a relevance score. One of the peculiarities is that it didn't depend on the actual query. Instead, it simply helped to order the results that contained the query terms. Nowadays search engines use machine learning and behavioral models to obtain query-dependent relevance scores. There are entire conferences devoted to this subject. \n",
        "\n<!-- Add / clean up-->"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommender systems\n",
        "\n",
        "Recommender systems are another problem setting that is related to search and ranking. The problems are  similar insofar as the goal is to display a set of relevant items to the user. The main difference is the emphasis on *personalization* to specific users in the context of recommender systems. For instance, for movie recommendations, the results page for a SciFi fan and the results page for a connoisseur of Woody Allen comedies might differ significantly. \n",
        "\n",
        "Such problems occur, e.g. for movie, product or music recommendation. In some cases, customers will provide explicit details about how much they liked the product (e.g. Amazon product reviews). In some other cases, they might simply provide feedback if they are dissatisfied with the result (skipping titles on a playlist). Generally, such systems strive to estimate some score $y_{ij}$, such as an estimated rating or probability of purchase, given a user $u_i$ and product $p_j$. \n",
        "\n",
        "Given such a model, then for any given user, we could retrieve the set of objects  with the largest scores $y_{ij}$ are then used as a recommendation. Production systems are considerably more advanced and take detailed user activity and item characteristics into account when computing such scores. The following image is an example of deep learning books recommended by Amazon based on personalization algorithms tuned to the author's preferences.\n",
        "\n",
        "![](../img/deeplearning_amazon.png)\n"
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Learning\n",
        "\n",
        "So far we've looked at problems where we have some fixed number of inputs \n",
        "and produce a fixed number of outputs. \n",
        "Before we considered predicting home prices from a fixed set of features:\n",
        "square footage, number of bedrooms, number of bathrooms, walking time to downtown.\n",
        "We also discussed mapping from an image (of fixed dimension), \n",
        "to the predicted probabilities that it belongs to each of a fixed number of classes,\n",
        "or taking a user ID and a product ID, and predicting a star rating.\n",
        "In these cases, once we feed our fixed-length input into the model to generate an output, \n",
        "the model immediately forgets what it just saw. \n",
        "\n",
        "This might be fine if our inputs truly all have the same dimensions \n",
        "and if successive inputs truly have nothing to do with each other. \n",
        "But how would we deal with video snippets?\n",
        "In this case, each snippet might consist of a different number of frames.\n",
        "And our guess of what's going on in each frame\n",
        "might be much stronger if we take into account \n",
        "the previous or succeeding frames. \n",
        "Same goes for language. \n",
        "One popular deep learning problem is machine translation:\n",
        "the task of ingesting sentences in some source language \n",
        "and predicting their translation in another language.\n",
        "\n",
        "These problems also occur in medicine.\n",
        "We might want a model to monitor patients in the intensive care unit and to fire off alerts \n",
        "if their risk of death in the next 24 hours exceeds some threshold. \n",
        "We definitely wouldn't want this model to throw away everything it knows about the patient history each hour, \n",
        "and just make its predictions based on the most recent measurements. \n",
        "\n",
        "These problems are among the more exciting applications of machine learning\n",
        "and they are instances of *sequence learning*. \n",
        "They require a model to either ingest sequences of inputs \n",
        "or to emit sequences of outputs (or both!).\n",
        "These latter problems are sometimes referred to as ``seq2seq`` problems. \n",
        "Language translation is a ``seq2seq`` problem. \n",
        "Transcribing text from spoken speech is also a ``seq2seq`` problem.\n",
        "While it is impossible to consider all types of sequence transformations, \n",
        "a number of special cases are worth mentioning:\n",
        "\n",
        "#### Tagging and Parsing\n",
        "\n",
        "This involves annotating a text sequence with attributes. In other words, the number of inputs and outputs is essentially the same. For instance, we might want to know where the verbs and subjects are. Alternatively, we might want to know which words are the named entities. In general, the goal is to decompose and annotate text based on structural and grammatical assumptions to get some annotation. This sounds more complex than it actually is. Below is a very simple example of annotating a sentence with tags indicating which words refer to named entities. \n",
        "\n\n",
        "|Tom | wants | to | have | dinner | in | Washington | with | Sally.|\n",
        "|:--|\n",
        "|Ent | - | - | - | - | - | Ent | - | Ent|\n",
        "\n\n",
        "#### Automatic Speech Recognition\n",
        "\n",
        "With speech recognition, the input sequence $x$ is the sound of a speaker, \n",
        "and the output $y$ is the textual transcript of what the speaker said. \n",
        "The challenge is that there are many more audio frames (sound is typically sampled at 8kHz or 16kHz) than text, i.e. there is no 1:1 correspondence between audio and text,\n",
        "since thousands of samples correspond to a single spoken word. \n",
        "These are seq2seq problems where the output is much shorter than the input. \n",
        "\n",
        "|`----D----e----e-----p------- L----ea------r------ni-----ng---`|\n",
        "|:--------------|\n",
        "|![Deep Learning](../img/speech.jpg)|\n",
        "\n",
        "#### Text to Speech\n",
        "\n",
        "Text to Speech (TTS) is the inverse of speech recognition.\n",
        "In other words, the input $x$ is text \n",
        "and the output $y$ is an audio file. \n",
        "In this case, the output is *much longer* than the input. \n",
        "While it is easy for *humans* to recognize a bad audio file, \n",
        "this isn't quite so trivial for computers. \n",
        "\n",
        "#### Machine Translation\n",
        "\n",
        "Unlike the case of speech of recognition, where corresponding inputs and outputs occur in the same order (after alignment), \n",
        "in machine translation, order inversion can be vital. \n",
        "In other words, while we are still converting one sequence into another, \n",
        "neither the number of inputs and outputs \n",
        "nor the order of corresponding data points \n",
        "are assumed to be the same.\n",
        "Consider the following illustrative example of the obnoxious tendency of Germans \n",
        "(*Alex writing here*) \n",
        "to place the verbs at the end of sentences. \n",
        "\n",
        "|German |Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?|\n",
        "|:------|:---------|\n",
        "|English|Did you already check out this excellent tutorial?|\n",
        "|Wrong alignment |Did you yourself already this excellent tutorial looked-at?|\n",
        "\n",
        "A number of related problems exist. \n",
        "For instance, determining the order in which a user reads a webpage \n",
        "is a two-dimensional layout analysis problem. \n",
        "Likewise, for dialogue problems, \n",
        "we need to take world-knowledge and prior state into account. \n",
        "This is an active area of research.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised learning\n",
        "\n",
        "All the examples so far were related to *Supervised Learning*, \n",
        "i.e. situations where we feed the model \n",
        "a bunch of examples and a bunch of *corresponding target values*.\n",
        "You could think of supervised learning as having an extremely specialized job and an extremely anal boss. \n",
        "The boss stands over your shoulder and tells you exactly what to do in every situation until you learn to map from situations to actions.\n",
        "Working for such a boss sounds pretty lame. \n",
        "On the other hand, it's easy to please this boss. You just recognize the pattern as quickly as possible and imitate their actions. \n",
        "\n",
        "In a completely opposite way,\n",
        "it could be frustrating to work for a boss \n",
        "who has no idea what they want you to do.\n",
        "However, if you plan to be a data scientist, you had better get used to it. \n",
        "The boss might just hand you a giant dump of data and tell you to *do some data science with it!*\n",
        "This sounds vague because it is. \n",
        "We call this class of problems *unsupervised learning*, \n",
        "and the type and number of questions we could ask \n",
        "is limited only by our creativity. \n",
        "We will address a number of unsupervised learning techniques in later chapters. To whet your appetite for now, we describe a few of the questions you might ask:\n",
        "\n",
        "* Can we find a small number of prototypes that accurately summarize the data? Given a set of photos, can we group them into landscape photos, pictures of dogs, babies, cats, mountain peaks, etc.? Likewise, given a collection of users' browsing activity, can we group them into users with similar behavior? This problem is typically known as **clustering**.\n",
        "* Can we find a small number of parameters that accurately capture the relevant properties of the data? The trajectories of a ball are quite well described by velocity, diameter, and mass of the ball. Tailors have developed a small number of parameters that describe human body shape fairly accurately for the purpose of fitting clothes. These problems are referred to as **subspace estimation** problems. If the dependence is linear, it is called **principal component analysis**.\n",
        "* Is there a representation of (arbitrarily structured) objects in Euclidean space (i.e. the space of vectors in $\\mathbb{R}^n$) such that symbolic properties can be well matched? This is called **representation learning** and it is used to describe entities and their relations, such as Rome - Italy + France = Paris. \n",
        "* Is there a description of the root causes of much of the data that we observe? For instance, if we have demographic data about house prices, pollution, crime, location, education, salaries, etc., can we discover how they are related simply based on empirical data? The field of **directed graphical models** and **causality** deals with this.\n",
        "* An important and exciting recent development is **generative adversarial networks**. They are basically a procedural way of synthesizing data. The underlying statistical mechanisms are tests to check whether real and fake data are the same. We will devote a few notebooks to them. \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interacting with an environment\n",
        "\n",
        "So far, we haven't discussed where data actually comes from, \n",
        "or what actually *happens* when a machine learning model generates an output. \n",
        "That's because supervised learning and unsupervised learning\n",
        "do not address these issues in a very sophisticated way.\n",
        "In either case, we grab a big pile of data up front,\n",
        "then do our pattern recognition without ever interacting with the environment again.\n",
        "Because all of the learning takes place after the algorithm is disconnected from the environment,\n",
        "this is called *offline learning*.\n",
        "For supervised learning, the process looks like this:\n",
        "\n",
        "![](../img/data-collection.png)\n",
        "\n\n",
        "This simplicity of offline learning has its charms.\n",
        "The upside is we can worry about pattern recognition in isolation without these other problems to deal with,\n",
        "but the downside is that the problem formulation is quite limiting.\n",
        "If you are more ambitious, or if you grew up reading Asimov's Robot Series,\n",
        "then you might imagine artificially intelligent bots capable not only of making predictions,\n",
        "but of taking actions in the world. \n",
        "We want to think about intelligent *agents*, not just predictive *models*. \n",
        "That means we need to think about choosing *actions*, not just making *predictions*.\n",
        "Moreover, unlike predictions, actions actually impact the environment. \n",
        "If we want to train an intelligent agent,\n",
        "we must account for the way its actions might \n",
        "impact the future observations of the agent. \n",
        "\n\n",
        "Considering the interaction with an environment that opens a whole set of new modeling questions. Does the environment:\n",
        "\n",
        "* remember what we did previously?\n",
        "* want to help us, e.g. a user reading text into a speech recognizer?\n",
        "* want to beat us, i.e. an adversarial setting like spam filtering (against spammers) or playing a game (vs an opponent)?\n",
        "* not  care (as in most cases)?\n",
        "* have shifting dynamics (steady vs shifting over time)?\n",
        "\n",
        "This last question raises the problem of *covariate shift*,\n",
        "(when training and test data are different). \n",
        "It's a problem that most of us have experienced when taking exams written by a lecturer,\n",
        "while the homeworks were composed by his TAs. \n",
        "We'll briefly describe reinforcement learning, and adversarial learning, \n",
        "two settings that explicitly consider interaction with an environment. \n",
        "\n\n",
        "### Reinforcement learning\n",
        "\n",
        "If you're interested in using machine learning to develop an agent that interacts with an environment and takes actions, then you're probably going to wind up focusing on *reinforcement learning* (RL). \n",
        "This might include applications to robotics, to dialogue systems, \n",
        "and even to developing AI for video games. \n",
        "*Deep reinforcement learning* (DRL), which applies deep neural networks\n",
        "to RL problems, has surged in popularity. \n",
        "The breakthrough [deep Q-network that beat humans at Atari games using only the visual input](https://www.wired.com/2015/02/google-ai-plays-atari-like-pros/) ,\n",
        "and the [AlphaGo program that dethroned the world champion at the board game Go](https://www.wired.com/2017/05/googles-alphago-trounces-humans-also-gives-boost/) are two prominent examples.\n",
        "\n",
        "Reinforcement learning gives a very general statement of a problem,\n",
        "in which an agent interacts with an environment over a series of *time steps*.\n",
        "At each time step $t$, the agent receives some observation $o_t$ from the environment,\n",
        "and must choose an action $a_t$ which is then transmitted back to the environment. \n",
        "Finally, the agent receives a reward $r_t$ from the environment.\n",
        "The agent then receives a subseqeunt observation, and chooses a subsequent action, and so on.\n",
        "The behavior of an RL agent is governed by a *policy*.\n",
        "In short, a *policy* is just a function that maps from observations (of the environment) to actions.\n",
        "The goal of reinforcement learning is to produce a good policy.\n",
        "\n",
        "![](../img/rl-environment.png)\n",
        "\n",
        "It's hard to overstate the generality of the RL framework.\n",
        "For example, we can cast any supervised learning problem as an RL problem. \n",
        "Say we had a classification problem. \n",
        "We could create an RL agent with one *action* corresponding to each class. \n",
        "We could then create an environment which gave a reward \n",
        "that was exactly equal to the loss function from the original supervised problem.\n",
        "\n",
        "That being said, RL can also address many problems that supervised learning cannot. \n",
        "For example, in supervised learning we always expect\n",
        "that the training input comes associated with the correct label.\n",
        "But in RL, we don't assume that for each observation, \n",
        "the environment tells us the optimal action.\n",
        "In general, we just get some reward.\n",
        "Moreover, the environment may not even tell us which actions led to the reward. \n",
        "\n",
        "Consider for example the game of chess. \n",
        "The only real reward signal comes at the end of the game when we either win, which we might assign a reward of 1,\n",
        "or when we lose, which we could assign a reward of -1.\n",
        "So reinforcement learners must deal with the *credit assignment problem*.\n",
        "The same goes for an employee who gets a promotion on October 11. \n",
        "That promotion likely reflects a large number of well-chosen actions over the previous year.\n",
        "Getting more promotions in the future requires figuring out what actions along the way led to the promotion.\n",
        "\n",
        "Reinforcement learners may also have to deal with the problem of partial observability. \n",
        "That is, the current observation might not tell you everything about your current state. \n",
        "Say a cleaning robot found itself trapped in one of many identical closets in a house.\n",
        "Inferring the precise location (and thus state) of the robot\n",
        "might require considering its previous observerations before entering the closet. \n",
        "\n",
        "Finally, at any given point, reinforcement learners might know of one good policy,\n",
        "but there might be many other better policies that the agent has never tried. \n",
        "The reinforcement learner must constantly choose \n",
        "whether to *exploit* the best currently-known strategy as a policy,\n",
        "or to *explore* the space of strategies, \n",
        "potentially giving up some short-run reward in exchange for knowledge.\n",
        "\n\n",
        "### MDPs, bandits, and friends\n",
        "\n",
        "The general reinforcement learning problem\n",
        "is a very general setting. \n",
        "Actions affect subsequent observations. \n",
        "Rewards are only observed corresponding to the chosen actions.\n",
        "The environment may be either fully or partially observed.\n",
        "Accounting for all this complexity at once may ask too much of researchers.\n",
        "Moreover not every practical problem exhibits all this complexity.\n",
        "As a result, researchers have studied a number of *special cases* of reinforcement learning problems. \n",
        "\n",
        "When the environment is fully observed, we call the RL problem a *Markov Decision Process* (MDP).\n",
        "When the state does not depend on the previous actions, \n",
        "we call the problem a *contextual bandit problem*. \n",
        "When there is no state, just a set of available actions with initially unknown rewards,\n",
        "this problem is the classic *multi-armed bandit problem*. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When *not* to use machine learning\n",
        "\n",
        "Let's take a closer look at the idea of programming data\n",
        "by considering an interaction that [Joel Grus](http://joelgrus.com) reported experiencing in a [job interview](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/). The interviewer asked him to code up Fizz Buzz. This is a children's game where the players count from 1 to 100 and will say *'fizz'* whenever the number is divisible by 3, *'buzz'* whenever it is divisible by 5, and *'fizzbuzz'* whenever it satisfies both criteria. Otherwise, they will just state the number. It looks like this:\n",
        "\n",
        "```\n",
        "1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizzbuzz 16 ...\n",
        "```\n",
        "\nThe conventional way to solve such a task is quite simple."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "res = []\n",
        "for i in range(1, 101):\n",
        "    if i % 15 == 0:\n",
        "        res.append('fizzbuzz')\n",
        "    elif i % 3 == 0:\n",
        "        res.append('fizz')\n",
        "    elif i % 5 == 0:\n",
        "        res.append('buzz')\n",
        "    else:\n",
        "        res.append(str(i))\n",
        "print(' '.join(res))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 14 fizzbuzz 16 17 fizz 19 buzz fizz 22 23 fizz buzz 26 fizz 28 29 fizzbuzz 31 32 fizz 34 buzz fizz 37 38 fizz buzz 41 fizz 43 44 fizzbuzz 46 47 fizz 49 buzz fizz 52 53 fizz buzz 56 fizz 58 59 fizzbuzz 61 62 fizz 64 buzz fizz 67 68 fizz buzz 71 fizz 73 74 fizzbuzz 76 77 fizz 79 buzz fizz 82 83 fizz buzz 86 fizz 88 89 fizzbuzz 91 92 fizz 94 buzz fizz 97 98 fizz buzz\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This isn't very exciting if you're a good programmer. Joel proceeded to 'implement' this problem in Machine Learning instead. For that to succeed, he needed a number of pieces:\n",
        "\n",
        "* Data X ``[1, 2, 3, 4, ...]`` and labels Y ``['fizz', 'buzz', 'fizzbuzz', identity]`` \n",
        "* Training data, i.e. examples of what the system is supposed to do. Such as ``[(2, 2), (6, fizz), (15, fizzbuzz), (23, 23), (40, buzz)]``\n",
        "* Features that map the data into something that the computer can handle more easily, e.g. ``x -> [(x % 3), (x % 5), (x % 15)]``. This is optional but helps a lot if you have it. \n",
        "\n",
        "Armed with this, Joel wrote a classifier in TensorFlow ([code](https://github.com/joelgrus/fizz-buzz-tensorflow)). The interviewer was nonplussed ... and the classifier didn't have perfect accuracy.\n",
        "\n",
        "Quite obviously, this is silly. Why would you go through the trouble of replacing a few lines of Python with something much more complicated and error prone? However, there are many cases where a simple Python script simply does not exist, yet a 3-year-old child will solve the problem perfectly. \n",
        "Fortunately, this is precisely where machine learning comes to the rescue. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\nMachine Learning is vast. We cannot possibly cover it all. On the other hand, neural networks are simple and only require elementary mathematics. So let's get started. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next\n",
        "[Manipulate data the MXNet way with NDArray](../chapter01_crashcourse/ndarray.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}