{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# まえがき\n",
        "\n",
        "多くの人は、このページのタイトルにある*mxnet-the-straight-dope*が何なのか疑問をもつと思います。また、ある人はなぜそのような大げさな名前をつけたのかと疑問に思うと思います。まず最初の質問に関してですが、 *mxnet-the-straight-dope*はDeep Learningのための新しい学習リソースを作ろう、という意味が込められています。ここでの目的は、ノートブックを利用することで、文章、画像、数式、そして（特に重要なものとして）コードを1箇所にまとめて説明することです。ここでの取り組みがうまくいけば、その成果は書籍、教材、チュートリアルの一部、有用かつ流用可能なコードになると思います。私達の知りうる限りでは、(1) 最新の Deep Learning の幅広いコンセプトと (2) 実行可能なコードを容易に利用できるノートブック、これらの双方を提供するものはいままでありませんでした。この試みを行う特別な理由がいまのところ見つかっていないとしても、いずれ、その理由が見つかると考えています。\n",
        "<!-- \n",
        "If you're a reasonable person, you might ask, \"what is *mxnet-the-straight-dope*?\" You might also ask, \"why does it have such an ostentatious name?\" Speaking to the former question, *mxnet-the-straight-dope* is an attempt to create a new kind of educational resource for deep learning. Our goal is to leverage the strengths of Jupyter notebooks to present prose, graphics, equations, and (importantly) code together in one place. If we're successful, the result will be a resource that could be simultaneously a book, course material, a prop for live tutorials, and a resource for plagiarising (with our blessing) useful code. To our knowledge, few available resources aim to teach either (1) the full breadth of concepts in modern machine learning or (2) interleave an engaging textbook with runnable code. We'll find out by the end of this venture whether or not that void exists for a good reason. -->\n",
        "\n",
        "名前に関してですが、私達が所属する機械学習のコミュニティとエコシステムは不合理なものになっていると感じています。2000年代のはじめ、機械学習で解けたと言われるタスクはほとんど無かったと思いますが、それらの機械学習モデルがどのように、なぜ動いているかを、一定の条件付きで理解できていたと思います。対照的に、今日の機械学習システムはとてつもなく強力で、様々なタスクに対しても実用的なレベルで適用できます。しかし、なぜ効果的かを正確に説明するためには膨大な質問に回答していく必要があります。\n",
        "<!-- \n",
        "Regarding the name, we are cognizant that the machine learning community and the ecosystem in which we operate have lurched into an absurd place. In the early 2000s, comparatively few tasks in machine learning had been conquered, but we felt that we understood *how* and *why* those models worked (with some caveats). By contrast, today's machine learning systems are extremely powerful and *actually work* for a growing list of tasks, but huge open questions remain as to precisely *why* they are so effective.   -->\n",
        "\n",
        "この新しい世界は多くの機会を与えてくれますが、一方で非常におどけたものにもなっています。\n",
        "[arXiv](http://arxiv.org)と呼ばれるプレプリントのサーバは見た人をそそのかすようなコンテンツであふれており、AIのスタートアップは過度に楽観的な評価を受けていて、ブログ界隈も技術的な知識のないマーケターによって書かれたリーダーシップのかけらのようなものにあふれています。混沌とお金とゆるい基準のなかで、われわれのやり方をとるか、非常に崇拝されているマーケターの環境をとるかは重要ではないと考えています。また、私達が取り組みたい様々なモデルを説明し、可視化し、実装するためには、執筆者が書くことに退屈しないということが重要です。\n",
        "<!-- \n",
        "This new world offers enormous opportunity, but has also given rise to considerable buffoonery. Research preprints like [the arXiv](http://arxiv.org) are flooded by clickbait, AI startups have sometimes received overly optimistic valuations, and the blogosphere is flooded with thought leadership pieces written by marketers bereft of any technical knowledge. Amid the chaos, easy money, and lax standards, we believe it's important not to take our models or the environment in which they are worshipped too seriously. Also, in order to both explain, visualize, and code the full breadth of models that we aim to address, it's important that the authors do not get bored while writing.  -->\n",
        "\n",
        "## 構成\n",
        "\n",
        "現在のところ、次のようなフォーマットに従おうとしています。基本的な数学的知識に関する少数のノートブックを除いて、各ノートブックは:\n",
        "\n",
        "1. 適切な数（大抵の場合1つ）の新しいコンセプトを紹介します\n",
        "2. 実際のデータセットを用いて、自分ひとりでも試せるようなサンプルを提供します。\n",
        "\n",
        "<!-- \n",
        "At present, we're aiming for the following format: aside from a few (optional) notebooks providing a crash course in the basic mathematical background, each subsequent notebook will both:\n",
        "\n",
        "1. Introduce a reasonable number (perhaps one) of new concepts\n",
        "2. Provide a single self-contained working example, using a real dataset -->\n",
        "\n",
        "このことは構成上の挑戦を表しています。1つのノートブックで、複数のモデルを論理的にグループ化して紹介するほうがいいかもしれません。また、様々なモデルを連続して実行させて学習することが良いかもしれません。一方で、1つのノートブックに1つのサンプルを置くという方針にこだわることは大きな利点があります。ここにあるコードを再利用して、自分自身の研究を始めやすくすることができます。1つのノートブックをコピーして、それを修正するだけです。\n",
        "\n",
        "<!-- \n",
        "This presents an organizational challenge. Some models might logically be grouped together in a single notebook. \n",
        "And some ideas might be best taught by executing several models in succession. \n",
        "On the other hand, there's a big advantage to adhering to a policy of *1 working example, 1 notebook*:\n",
        "This makes it as easy as possible for you to start your own research projects \n",
        "by plagiarising our code. Just copy a single notebook and start modifying it.\n",
        " -->\n",
        "\n",
        "必要に応じて背景的な情報の中に実行可能なコードを差し込みます。ここでの一般的な方針として、ツールそのものの説明を完全に行う前に、ツールを利用できるように注力します。そして、後に背景情報を説明してフォローアップもします。例えば、*stochastic gradient descent*がなぜ有用か、どのように動いているのかを十分に説明する前に、*stochastic gradient descent*を使うかもしれません。このことは、読者の私達に対する信用を犠牲にするかもしれませんが、実務者に対して問題をすばやく解決するための情報を与えます。全体を通して、ここではMXNetのライブラリを利用しています。MXNetは本番環境でも十分に速く、研究活動にも柔軟であるという得難い特徴をもっています。より発展的な章では、ハイレベルでシンプルなインタフェースである``Gluon``を利用します。これは、MXNEtがサポートしている古いシンボリックなインタフェースである``mxnet.module``とは異なっています。\n",
        " \n",
        "<!-- We will interleave the runnable code with background material as needed. \n",
        "In general, we will often err on the side of making tools available before explaining them fully \n",
        "(and we will follow up by explaining the background later). \n",
        "For instance, we might use *stochastic gradient descent* \n",
        "before fully explaining why it is useful or why it works. \n",
        "This helps to give practitioners the necessary ammunition to solve problems quickly, \n",
        "at the expense of requiring the reader to trust us with some decisions, at least in the short term. \n",
        "Throughout, we'll be working with the MXNet library, \n",
        "which has the rare property of being flexible enough for research \n",
        "while being fast enough for production. \n",
        "Our more advanced chapters will mostly rely \n",
        "on MXNet's new high-level imperative interface ``gluon``. \n",
        "Note that this is not the same as ``mxnet.module``, \n",
        "an older, symbolic interface supported by MXNet.  -->\n",
        "\n",
        "このコンテンツでは、Deep Learningのコンセプトを最初からお伝えします。``Gluon``によって隠蔽されたモデルについて、詳細を掘り下げようとすることもあります。特に基礎的なチュートリアルにおいて、こういったことがありますが、何が起こっているかを理解してほしいという目的があります。\n",
        "これらの場合において、通常、以下の2つのバージョンの例を用意しています。一つは、NDArrayや自動微分を利用してゼロから全て実装したもの、もう一つは、``Gluon``でそれらを簡潔にできることを示すことです。一度、あるレイヤーがどのように動いているかを示したら、その``Gluon``バージョンを次のチュートリアルで示します。\n",
        "\n",
        "<!-- \n",
        "This book will teach deep learning concepts from scratch. \n",
        "Sometimes, we'll want to delve into fine details about the models \n",
        "that are hidden from the user by ``gluon``'s advanced features. \n",
        "This comes up especially in the basic tutorials, \n",
        "where we'll want you to understand everything that happens in a given layer. \n",
        "In these cases, we'll generally present two versions of the example: \n",
        "one where we implement everything from scratch, \n",
        "relying only on NDArray and automatic differentiation, \n",
        "and another where we show how to do things succinctly with ``gluon``. \n",
        "Once we've taught you how a layer works, \n",
        "we can just use the ``gluon`` version in subsequent tutorials.\n",
        " -->\n",
        " \n",
        "## 実践して学習する\n",
        "\n",
        "多くの教科書はいくつかのトピックに渡っていて、個々の詳細は網羅的に記述されています。例えば、Chris Bishopの著名な本 [Pattern Recognition and Machine Learning](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)では、各トピックについてとても入念に説明されており、線形回帰の章に入るにしても、たくさんのことを学習します。このコンテンツの著者であるZackが最初に機械学習を学んだとき、初心者向けの本として、この本の有用性は限定されているように感じました。2,3年後にこの本に再会したときには、その徹底ぶりが非常に気に入りました。読者がこのコンテンツを一通り学んだ後は、ぜひ彼の本を読んでみてほしいと思う。しかし、伝統的な教科書によるアプローチが、最初に学び始める際に、最も簡単な方法であるとは限らない。\n",
        "\n",
        "<!-- \n",
        "teaches each topic so thoroughly, that getting to the chapter on linear regression requires a non-trivial amount of work. When I (Zack) was first learning machine learning, this actually limited the book's usefulness as an introductory text. When I rediscovered it a couple years later, I loved it precisely for its thoroughness, and I hope you check it out after working through this material! But perhaps the traditional textbook aproach is not the easiest way to get started in the first place.  -->\n",
        "\n",
        "代わりに、このコンテンツでは、ほとんどのコンテンツを適切なタイミングで提供します。線形代数や確率といった基礎的な内容のために、最初に短い学習用コンテンツを提供する。しかし、やや風変わりな確率分布の心配をする前に、機械学習のモデルを学習してみて、その満足感を味わってほしいと思う。\n",
        "<!-- \n",
        "Instead, in this book, we'll teach most concepts just in time. For \n",
        "the fundamental preliminaries like linear algebra and probability, \n",
        "we'll provide a brief crash course from the outset, \n",
        "but we want you to taste the satisfaction of training your first model \n",
        "before worrying about exotic probability distributions. \n",
        " -->\n",
        "## 次のステップ\n",
        "\n",
        "もし始める準備ができたなら、[はじめに](../chapter01_crashcourse/introduction.ipynb)やMXNetで動くデータ構造である[NDArrayにおける基礎](./ndarray.ipynb)に進みましょう。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
